# -*- coding: utf-8 -*-
"""phase 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hGCEknfw2giErOf_yjxAqhQuaN0pPVq_
"""

#import pandas library
import pandas as pd
df = pd.read_csv('news.csv', on_bad_lines='skip')

#read the dataset with name "Fake_Real_Data.csv" and store it in a variable df
# Adding error_bad_lines=False will skip the problematic row.
# Alternatively, quoting=3 would tell the parser to interpret quotes differently

#print the shape of dataframe
df.shape

#print top 5 rows
df.head(5)

#check the distribution of labels
df.label.value_counts()

# Since it is imbalanced, select any balancing technique to make it balanced.
#I've choosen undersampling. Anyhow it's almost equal. You can skip balancing technique also

min_sampling= 3164
df_real= df[df.label == 'REAL'].sample(min_sampling, random_state= 2022)
df_fake= df[df.label == 'FAKE'].sample(min_sampling, random_state= 2022)

df_balanced= pd.concat([df_fake, df_real], axis=0)
df_fake.shape

df_real.shape

df_balanced.label.value_counts()

#Add the new column "label_num" which gives a unique number to each of these labels
df_balanced['label_num'] = df_balanced['label'].apply(lambda x:1 if x== 'REAL' else 0)

#check the results with top 5 rows
df_balanced.head(5)
df_balanced.tail(5)

"""# Modelling without Pre-processing Text data"""

# import train-test-split from sklearn
import sklearn
from sklearn.model_selection import train_test_split

# Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too
X_train, X_test, y_train, y_test= train_test_split(df_balanced.label, df_balanced.label_num,
                                                   test_size=0.2, random_state= 20,
                                                   stratify= df_balanced.label_num)

# print the shapes of X_train and X_test
X_train.shape

X_test.shape

"""# create a classification pipeline to classify the Data"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer()],
               ['nb', MultinomialNB()]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

# get the predictions for X_test and store it in y_pred
y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))

# Attempt 2 :

# using the sklearn pipeline module create a classification pipeline to classify the Data.
# Note:

# using CountVectorizer with unigram, bigram, and trigrams.
# use KNN as the classifier with n_neighbors of 10 and metric as 'cosine' distance.
# print the classification report.

# Unigram - KNeighborsClassifier

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer(ngram_range=(1, 1))],
               ['KNN', KNeighborsClassifier(n_neighbors=10, metric = 'euclidean')]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))

# bigram - KNeighborsClassifier

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer(ngram_range=(1,2))],
               ['KNN', KNeighborsClassifier()]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))

# trigram - KNeighborsClassifier

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer(ngram_range=(1,3))],
               ['KNN', KNeighborsClassifier(n_neighbors = 10, metric = 'cosine')]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))

# Attempt 3 :

# using the sklearn pipeline module create a classification pipeline to classify the Data.
# Note:

# using CountVectorizer with only trigrams.
# use RandomForest as the classifier.
# print the classification report.

# trigram - RandomForestClassifier

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer(ngram_range=(1,3))],
               ['RF', RandomForestClassifier()]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))

# Bigram - naive_bayes
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer(ngram_range=(1,2))],
               ['nb', MultinomialNB()]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))

# Unigram - naive_bayes
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer(ngram_range=(1,1))],
               ['nb', MultinomialNB(alpha=0.75)]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))

"""# Modelling after Pre-processing Text data"""

import spacy

nlp= spacy.load("en_core_web_sm")
filtered_token= []
def preprocess(text):
    doc= nlp(text)
    for token in doc:
        if token.is_punct or token.is_stop:
            continue
        filtered_token.append(token.lemma_)
    return " ".join(filtered_token)

# create a new column "preprocessed_txt" and use the utility function above to get the clean data
# this will take some time, please be patient

df_balanced['processed_text'] = df_balanced.text.apply(preprocess)

df_balanced.head(5)

# Build a model with pre processed text
# import train-test-split from sklearn
import sklearn
from sklearn.model_selection import train_test_split


# Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too
X_train, X_test, y_train, y_test= train_test_split(df_balanced.label, df_balanced.label_num,
                                                   test_size=0.2, random_state= 20,
                                                   stratify= df_balanced.label_num)

X_train.shape

X_test.shape

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report


# create a pipeline object
clf = Pipeline([['Vectorization', CountVectorizer()],
               ['nb', MultinomialNB()]])

# fit with X_train and y_train
clf.fit(X_test, y_test)

# predict using X_test
y_pred = clf.predict(X_test)

# print the classfication report
print(classification_report(y_test,
    y_pred))



